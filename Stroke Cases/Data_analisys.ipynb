{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "import os\n",
    "spark = (SparkSession.builder\n",
    "         .appName('Stroke Cases')\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
      "|  0|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level|  bmi| smoking_status|stroke|\n",
      "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
      "|  1|Female|18.0|           0|            0|          No|      Private|         Urban|            94.19|12.12|         smokes|     1|\n",
      "|  2|  Male|58.0|           1|            0|         Yes|      Private|         Rural|           154.24| 33.7|   never_smoked|     0|\n",
      "|  3|Female|36.0|           0|            0|         Yes|     Govt_job|         Urban|            72.63| 24.7|         smokes|     0|\n",
      "|  4|Female|62.0|           0|            0|         Yes|Self-employed|         Rural|            85.52| 31.2|formerly smoked|     0|\n",
      "|  5|Female|82.0|           0|            0|         Yes|      Private|         Rural|            59.32| 33.2|         smokes|     1|\n",
      "|  6|Female|82.0|           0|            0|          No|     Govt_job|         Urban|            234.5| 24.0|formerly smoked|     0|\n",
      "|  7|Female|33.0|           0|            0|         Yes|Self-employed|         Urban|           193.42| 29.9|         smokes|     0|\n",
      "|  8|Female|37.0|           0|            0|         Yes|      Private|         Rural|            156.7| 36.9|         smokes|     1|\n",
      "|  9|Female|41.0|           0|            0|         Yes|     Govt_job|         Rural|            64.06| 33.8|         smokes|     1|\n",
      "| 10|Female|70.0|           0|            0|         Yes|Self-employed|         Rural|            76.34| 24.4|formerly smoked|     1|\n",
      "| 11|Female|25.0|           0|            0|          No|      Private|         Urban|            91.15| 28.7|         smokes|     1|\n",
      "| 12|Female|43.0|           1|            0|          No|Self-employed|         Rural|            60.12| 34.2|formerly smoked|     0|\n",
      "| 13|  Male|72.0|           0|            1|         Yes|      Private|         Rural|           235.22| 40.3|formerly smoked|     1|\n",
      "| 14|Female|20.0|           0|            0|          No|      Private|         Rural|           106.47| 33.7|         smokes|     1|\n",
      "| 15|  Male|20.0|           0|            0|          No|      Private|         Urban|           104.78| 20.3|         smokes|     1|\n",
      "| 16|  Male|41.0|           0|            0|         Yes|Self-employed|         Urban|            159.3| 34.6|         smokes|     1|\n",
      "| 17|Female|23.0|           0|            0|          No|      Private|         Urban|           116.95| 23.8|         smokes|     1|\n",
      "| 18|  Male|22.0|           0|            0|          No|Self-employed|         Rural|            72.05| 31.9|         smokes|     1|\n",
      "| 19|  Male|69.0|           0|            0|         Yes|      Private|         Rural|            64.06| 35.1|formerly smoked|     0|\n",
      "| 20|Female|44.0|           0|            0|         Yes|Self-employed|         Rural|           135.03| 36.1|         smokes|     1|\n",
      "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Parametrização e leitura do arquivo de analise com Spark\n",
    "\n",
    "hist_name = 'stroke_data.csv'\n",
    "\n",
    "file_name = os.path.join(os.getcwd(), hist_name)\n",
    "\n",
    "df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 12\n",
      "Column types:\n",
      "0: int\n",
      "gender: string\n",
      "age: double\n",
      "hypertension: int\n",
      "heart_disease: int\n",
      "ever_married: string\n",
      "work_type: string\n",
      "Residence_type: string\n",
      "avg_glucose_level: double\n",
      "bmi: double\n",
      "smoking_status: string\n",
      "stroke: int\n"
     ]
    }
   ],
   "source": [
    "# Obter o número de colunas\n",
    "num_cols = len(df.columns)\n",
    "\n",
    "# Obter o tipo de dados de cada coluna\n",
    "col_types = [col.dataType.simpleString() for col in df.schema]\n",
    "\n",
    "# Imprimir o número de colunas e o tipo de dados de cada coluna\n",
    "print(f\"Number of columns: {num_cols}\")\n",
    "print(\"Column types:\")\n",
    "for col, col_type in zip(df.columns, col_types):\n",
    "    print(f\"{col}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analise exploratória dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1|40287|\n",
      "|     0|26848|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Quantidade de pacientes que sofreram e não sofreram derrame (stroke), respectivamente.\n",
    "df.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|    work_type|count(stroke)|\n",
      "+-------------+-------------+\n",
      "|      Private|        23711|\n",
      "|Self-employed|        10807|\n",
      "|     Govt_job|         5164|\n",
      "|     children|          520|\n",
      "| Never_worked|           85|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Analise com uma tabela para identificar os parcientes que tiveram derrame por tipo de trabalho.\n",
    "df.createOrReplaceTempView('table')\n",
    "stroke_type = df.filter(df['stroke']== 1)\n",
    "by_work_type = stroke_type.groupBy('work_type').agg({'stroke': 'count'}).distinct()\n",
    "sort_work_type = by_work_type.orderBy('count(stroke)', ascending=False)\n",
    "sort_work_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+\n",
      "|gender|count|          Proportion|\n",
      "+------+-----+--------------------+\n",
      "|Female|39530|  58.881358456840694|\n",
      "| Other|   11|0.016384896104863336|\n",
      "|  Male|27594|   41.10225664705444|\n",
      "+------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Proporção do genero da população coletada\n",
    "\n",
    "gender_query = df.groupBy('gender').count()\n",
    "total_type = df.count()\n",
    "proportion_bygroup = gender_query.withColumn('Proportion',(gender_query['count']/total_type)*100)\n",
    "proportion_bygroup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupo Hypertenso:\n",
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1| 8817|\n",
      "+------+-----+\n",
      "\n",
      "Grupo Não-Hypertenso:\n",
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1|31470|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Analise de derrame por população(Hipertenso e não hipertenso)\n",
    "\n",
    "strokes=df.filter(df['stroke']==1)\n",
    "\n",
    "#Grupo 1\n",
    "hypertension_df1 = strokes.filter(df['hypertension']==1)\n",
    "group_stroke1 = hypertension_df1.groupBy('stroke').count()\n",
    "print('Grupo Hypertenso:')\n",
    "group_stroke1.show()\n",
    "\n",
    "#Grupo 2\n",
    "print('Grupo Não-Hypertenso:')\n",
    "hypertension_df2 = strokes.filter(df['hypertension']==0)\n",
    "group_stroke2 = hypertension_df2.groupBy('stroke').count()  \n",
    "group_stroke2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|79.0| 2916|\n",
      "|78.0| 2279|\n",
      "|80.0| 1858|\n",
      "|81.0| 1738|\n",
      "|82.0| 1427|\n",
      "|77.0|  994|\n",
      "|74.0|  987|\n",
      "|63.0|  942|\n",
      "|76.0|  892|\n",
      "|70.0|  881|\n",
      "|66.0|  848|\n",
      "|75.0|  809|\n",
      "|67.0|  801|\n",
      "|57.0|  775|\n",
      "|73.0|  759|\n",
      "|65.0|  716|\n",
      "|72.0|  709|\n",
      "|68.0|  688|\n",
      "|69.0|  677|\n",
      "|71.0|  667|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Derrames por idade\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "age_group = strokes.groupBy('age').count()\n",
    "age_group.orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|stroke|count|\n",
      "+------+-----+\n",
      "|     1|28938|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Derrames após os 50 anos.\n",
    "\n",
    "postfiftyes = strokes.filter(df['age'] > 50)\n",
    "postfiftyes.groupBy('stroke').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|stroke|avg(avg_glucose_level)|\n",
      "+------+----------------------+\n",
      "|     1|    119.95307046938272|\n",
      "|     0|    103.60273130214506|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Nível médio de glicose para pessoas que, respectivamente, sofreram e não sofreram derrame.avg_glucose_level\n",
    "\n",
    "glucose_level = df.groupBy('stroke').agg({'avg_glucose_level': 'avg'}) \n",
    "glucose_level.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|stroke|          avg(bmi)|\n",
      "+------+------------------+\n",
      "|     1|29.942490629729495|\n",
      "|     0|27.989678933253657|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BMI (IMC = índice de massa corpórea) médio de quem sofreu e não sofreu derrame.\n",
    "\n",
    "bmi = df.groupBy('stroke').agg({'bmi': 'avg'}) \n",
    "\n",
    "bmi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.6888000592197794\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Colunas selecionadas do DF para utilização do asembler\n",
    "pipeline_columns_df = ['age', 'bmi', 'hypertension', 'heart_disease', 'avg_glucose_level']\n",
    "\n",
    "# Criar um assembler para combinar as variáveis em um vetor de recursos\n",
    "assembler = VectorAssembler(inputCols= pipeline_columns_df, outputCol=\"features\")\n",
    "\n",
    "# Criar um modelo de árvore de decisão\n",
    "dt = DecisionTreeClassifier(labelCol=\"stroke\", featuresCol=\"features\")\n",
    "\n",
    "# Montar o pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Treinar o modelo\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Avaliar o modelo usando a métrica de acurácia\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Imprimir a acurácia\n",
    "print(\"Acurácia:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.8399408284023668\n"
     ]
    }
   ],
   "source": [
    "#Adição de duas novas variaveis para checar a melhora do algoritimo.\n",
    "\n",
    "# Indexar as colunas em formato string para numérico\n",
    "indexer1 = StringIndexer(inputCol=\"gender\", outputCol=\"indexed_gender\")\n",
    "indexer2 = StringIndexer(inputCol=\"smoking_status\", outputCol=\"indexed_smoking_status\")\n",
    "\n",
    "#Colunas selecionadas do DF para utilização do asembler\n",
    "pipeline_columns_df = ['age', 'bmi', 'hypertension', 'heart_disease', 'avg_glucose_level', 'indexed_smoking_status', 'indexed_gender']\n",
    "\n",
    "# Criar um assembler para combinar as variáveis em um vetor de recursos\n",
    "assembler = VectorAssembler(inputCols= pipeline_columns_df, outputCol=\"features\")\n",
    "\n",
    "# Criar um modelo de árvore de decisão\n",
    "dt = DecisionTreeClassifier(labelCol=\"stroke\", featuresCol=\"features\")\n",
    "\n",
    "# Montar o pipeline\n",
    "pipeline = Pipeline(stages=[indexer1, indexer2, assembler, dt])\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Treinar o modelo\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Avaliar o modelo usando a métrica de acurácia\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Imprimir a acurácia\n",
    "print(\"Acurácia:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importância da variável age: 0.13122909281688455\n",
      "Importância da variável bmi: 0.0\n",
      "Importância da variável hypertension: 0.0\n",
      "Importância da variável heart_disease: 0.0\n",
      "Importância da variável avg_glucose_level: 0.0075241927536957385\n",
      "Importância da variável indexed_smoking_status: 0.8612467144294197\n",
      "Importância da variável indexed_gender: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Verificação da importancia das variáveis da arvore de Decisão\n",
    "\n",
    "#Neste código, model.stages[-1] acessa a última etapa do pipeline, que é o modelo de árvore de decisão, e featureImportances retorna as importâncias das variáveis.\n",
    "\n",
    "feature_importances = model.stages[-1].featureImportances\n",
    "for i, importance in enumerate(feature_importances):\n",
    "    print(f\"Importância da variável {pipeline_columns_df[i]}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de nós na árvore de decisão: 11\n"
     ]
    }
   ],
   "source": [
    "# Acessar a quantidade de nós na árvore de decisão\n",
    "num_nodes = model.stages[-1].numNodes #Neste código, model.stages[-1] acessa a última etapa do pipeline, que é o modelo de árvore de decisão\n",
    "\n",
    "# Imprimir a quantidade de nós\n",
    "print(\"Quantidade de nós na árvore de decisão:\", num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obter a árvore de decisão do modelo\n",
    "tree_model = model.stages[-1]\n",
    "\n",
    "# Exportar a árvore de decisão para um formato que pode ser visualizado\n",
    "tree_dot = tree_model.toDebugString\n",
    "\n",
    "# Salvar a estrutura da árvore em um arquivo de texto\n",
    "with open('tree_structure.txt', 'w') as file:\n",
    "    file.write(tree_dot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
